# ğŸ¤– Asistente Virtual RAG Multimodal

**Asistente inteligente para la consulta de apuntes tÃ©cnicos en PDF e imÃ¡genes sobre Inteligencia Artificial y Big Data.**

Este proyecto implementa un sistema **RAG (Retrieval-Augmented Generation)** multimodal capaz de responder preguntas tÃ©cnicas complejas basÃ¡ndose en una base de conocimiento propia. Combina la capacidad de entender texto e imÃ¡genes para ofrecer respuestas precisas y fundamentadas.

---

## ğŸ¯ Objetivo

El objetivo principal es crear un asistente capaz de responder dudas tÃ©cnicas utilizando **documentos propios (PDFs) y diagramas/imÃ¡genes**, superando las limitaciones de los LLMs genÃ©ricos al inyectar contexto especÃ­fico actualizado.

---

## ğŸ—ï¸ Arquitectura

El sistema utiliza una arquitectura modular compuesta por:

* **Base de Datos Vectorial:** [ChromaDB](https://www.trychroma.com/) para el almacenamiento y recuperaciÃ³n eficiente de vectores.  
* **Embeddings:** `SentenceTransformers` para la vectorizaciÃ³n de texto e imÃ¡genes.  
* **LLM (GeneraciÃ³n):** IntegraciÃ³n flexible con **Groq**, **OpenAI** o **Ollama**.  
* **Backend:** [FastAPI](https://fastapi.tiangolo.com/) para la gestiÃ³n de la lÃ³gica y endpoints.  
* **Frontend:** [Streamlit](https://streamlit.io/) para una interfaz de usuario interactiva y amigable.

### âœ¨ CaracterÃ­sticas Principales

* âœ… **BÃºsqueda SemÃ¡ntica:** RecuperaciÃ³n inteligente en documentos PDF.  
* âœ… **Soporte Multimodal:** Capacidad para entender y recuperar imÃ¡genes/diagramas.  
* âœ… **Reranking:** Reordenamiento de resultados para mayor precisiÃ³n (Cross-Encoder).  
* âœ… **Query Rewriting:** ReformulaciÃ³n automÃ¡tica de preguntas con historial de chat.  
* âœ… **Transparencia:** VisualizaciÃ³n de las fuentes y documentos utilizados en cada respuesta.

---

## ğŸ“‚ Estructura del Proyecto

â”œâ”€â”€ chroma\_db\_multimodal(casa\_llava\_qwen)buena\_spanish/  \# Base de datos vectorial persistente

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ imagenes/          \# Dataset de imÃ¡genes

â”‚   â””â”€â”€ pdfs/              \# Dataset de documentos PDF

â”œâ”€â”€ img/                   \# Assets del proyecto (logos, avatares)

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ config.py          \# ConfiguraciÃ³n global

â”‚   â”œâ”€â”€ app/

â”‚   â”‚   â””â”€â”€ app.py         \# Frontend (Streamlit)

â”‚   â”œâ”€â”€ api/

â”‚   â”‚   â””â”€â”€ api.py         \# Backend (FastAPI)

â”‚   â”œâ”€â”€ 01\_multimodal\_ingest\_smart.py  \# Script de ingesta de imÃ¡genes

â”‚   â”œâ”€â”€ 02\_ingest\_pdfs.py              \# Script de ingesta de PDFs

â”‚   â”œâ”€â”€ 03\_check\_chroma\_content.py     \# Utilidad de verificaciÃ³n

â”‚   â”œâ”€â”€ 04\_resultados.py               \# VisualizaciÃ³n de resultados

â”‚   â”œâ”€â”€ 05\_comprobar.py                \# Tests A/B de texto

â”‚   â”œâ”€â”€ 05\_comprobar\_imagenes.py       \# Tests A/B de imÃ¡genes

â”‚   â”œâ”€â”€ 06\_buscar\_imagen.py            \# Buscador especÃ­fico de imÃ¡genes

â”‚   â”œâ”€â”€ 07\_eval\_retrieval.py           \# EvaluaciÃ³n de recuperaciÃ³n

â”‚   â”œâ”€â”€ 08\_ragas.py                    \# EvaluaciÃ³n con RAGAS

â”‚   â””â”€â”€ 09\_evaluar\_metricas.py         \# Benchmark de configuraciones

â”œâ”€â”€ .env                   \# Variables de entorno (API Keys)

â”œâ”€â”€ requirements.txt       \# Dependencias

â””â”€â”€ README.md              \# DocumentaciÃ³n

## InstalaciÃ³n

1. Crear entorno con conda: conda create \--name rag\_multimodal python=3.10 \-y conda activate rag\_multimodal  
     
2. Instalar dependencias: pip install \-r requirements.txt

## EjecuciÃ³n

1. Ejecutar la ingesta de imÃ¡genes (multimodal): python src/01\_multimodal\_ingest\_smart.py  
     
2. Ejecutar la ingesta de documentos PDF: python src/02\_ingest\_pdfs.py  
     
3. Lanzar la API: python api/api.py  
     
4. Lanzar la aplicaciÃ³n Streamlit: streamlit run app/app.py

## EvaluaciÃ³n del Sistema

Se han realizado distintas pruebas para evaluar el rendimiento del sistema RAG multimodal en tÃ©rminos de recuperaciÃ³n de informaciÃ³n (retrieval) y calidad semÃ¡ntica de las respuestas.

### ComparaciÃ³n de Modelos de Embeddings

Se compararon distintos modelos para analizar cuÃ¡l ofrece mejores resultados en espaÃ±ol:

**Resultados:**

- Modelo `multilingual-e5`: **80.00%**  
- Modelo `qwen`: **90.00%**

AdemÃ¡s, se evaluÃ³ el impacto del idioma de las descripciones de las imÃ¡genes (generado con CLIP):

- Modelo `qwen (no spanish)`: **85.00%**  
- Modelo `qwen (spanish)`: **95.00%**

**ConclusiÃ³n:**  
El modelo **Qwen con las descripciones de las imÃ¡genes en espaÃ±ol** obtiene el mejor rendimiento, confirmando la importancia de utilizar embeddings adaptados al idioma.

### EvaluaciÃ³n General de Retrieval (07\_eval\_retrieval.py)

Se ejecutÃ³ el script `07_eval_retrieval.py`, obteniendo:

- Hit Rate: 83.3%

Este resultado indica que el sistema recupera correctamente documentos relevantes en mÃ¡s del 80% de las consultas realizadas.

### EvaluaciÃ³n por Configuraciones (Chunk \+ Reranker)

Se evaluaron distintas configuraciones variando el tamaÃ±o de los fragmentos (chunk size) y el uso de reranker, midiendo Hit Rate@3, MRR@3 y latencia.

| ConfiguraciÃ³n | Hit Rate@3 | MRR@3 | Latencia (s) |
| :---- | :---- | :---- | :---- |
| db\_800 (Base) | 76.9% | 0.73 | 0.335 |
| db\_800 (+Reranker) | 84.6% | 0.77 | 5.083 |
| db\_1000 (Base) | 76.9% | 0.68 | 0.328 |
| db\_1000 (+Reranker) | 84.6% | 0.78 | 5.861 |

**Conclusiones:**

- El uso de **reranker mejora significativamente la precisiÃ³n** (hasta un 84.6% de Hit Rate@3).  
- Aumenta la latencia, por lo que existe un compromiso entre calidad y velocidad.  
- La configuraciÃ³n `db_1000 + reranker` obtiene el mejor MRR@3 (0.78).

### EvaluaciÃ³n SemÃ¡ntica con RAGAS

Para evaluar la calidad de las respuestas generadas se utilizÃ³ la librerÃ­a **RAGAS**, con las mÃ©tricas:

- Faithfulness (fidelidad al contexto)  
- Answer Relevancy (relevancia de la respuesta)  
- Context Precision (precisiÃ³n del contexto recuperado)

| Pregunta | Faithfulness | Answer Relevancy | Context Precision |
| :---- | :---- | :---- | :---- |
| 1\. Kafka | 1.000 | 0.895 | 0.633 |
| 2\. Componentes | 0.714 | 1.000 | 0.633 |
| 3\. Supervisado | 1.000 | 0.921 | 0.853 |
| **PROMEDIO** | **0.905** | **0.939** | **0.706** |

**Conclusiones:**

- El sistema presenta una alta **faithfulness (0.905)**, indicando que las respuestas estÃ¡n basadas en los documentos recuperados.  
- La **answer relevancy (0.939)** demuestra que las respuestas son adecuadas y coherentes con las preguntas.  
- La **context precision (0.706)** muestra un buen nivel de selecciÃ³n de fragmentos relevantes.

### ConclusiÃ³n Global

Los resultados obtenidos demuestran que el sistema RAG multimodal:

- Recupera informaciÃ³n relevante de forma eficaz.  
- Genera respuestas coherentes y fundamentadas.  
- Mejora su rendimiento al utilizar embeddings optimizados para espaÃ±ol y reranking.  
- Presenta un equilibrio razonable entre calidad semÃ¡ntica y latencia.

Este proceso de evaluaciÃ³n valida la robustez del sistema y justifica las decisiones tomadas en el diseÃ±o del pipeline.

## Autores

Proyecto realizado por ZuriÃ±e Colino y Aritz Monje.

# **Asistente Virtual RAG Multimodal: EspecializaciÃ³n en IA y Big Data**

**Sistema de RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG) con capacidades multimodales (Texto \+ Imagen) para la gestiÃ³n del conocimiento acadÃ©mico.**

## **1\. DescripciÃ³n del Proyecto**

Este repositorio contiene la implementaciÃ³n completa de un asistente virtual tÃ©cnico diseÃ±ado para resolver el problema de la fragmentaciÃ³n de la informaciÃ³n en el entorno universitario. El sistema permite a los estudiantes interactuar en lenguaje natural con una base de conocimiento curada, compuesta por apuntes tÃ©cnicos (PDFs), diagramas de arquitectura y diapositivas de clase (ImÃ¡genes).

A diferencia de los LLMs generalistas (como ChatGPT), este sistema opera bajo un esquema de **Dominio Cerrado**: las respuestas se generan exclusivamente a partir de la documentaciÃ³n indexada, eliminando las alucinaciones y garantizando la trazabilidad de la informaciÃ³n mediante citas explÃ­citas a las fuentes.

La soluciÃ³n integra un pipeline avanzado de **BÃºsqueda HÃ­brida** (SemÃ¡ntica \+ Palabras Clave) y un sistema de **Reordenamiento (Reranking)**, optimizado especÃ­ficamente para el idioma espaÃ±ol y terminologÃ­a tÃ©cnica de IngenierÃ­a de Datos.

### **1.1. MotivaciÃ³n y Problema a Resolver**

En asignaturas tÃ©cnicas como *Big Data* o *Inteligencia Artificial*, el material de estudio suele estar disperso en mÃºltiples formatos:

* **Texto denso:** Manuales de referencia y papers en PDF.  
* **InformaciÃ³n visual crÃ­tica:** Diagramas de flujo (ej. arquitectura Kafka), capturas de cÃ³digo y esquemas conceptuales que los LLMs de texto tradicionales ignoran.

**El problema:** Los estudiantes pierden tiempo buscando referencias cruzadas y los modelos estÃ¡ndar fallan al interpretar preguntas que requieren contexto visual especÃ­fico (ej. "Â¿QuÃ© representa el bloque azul en el diagrama de arquitectura de Hadoop?").

**Nuestra soluciÃ³n:** Un motor RAG Multimodal que vectoriza tanto el texto como las descripciones semÃ¡nticas de las imÃ¡genes, permitiendo una recuperaciÃ³n de informaciÃ³n holÃ­stica.

### **1.2. Objetivos Principales**

* **CentralizaciÃ³n del Conocimiento:** Unificar fuentes heterogÃ©neas en una Ãºnica base de datos vectorial consultable (ChromaDB).  
* **PrecisiÃ³n TÃ©cnica (Zero-Hallucination):** Implementar *Guardrails* estrictos en el prompt del sistema para restringir las respuestas Ãºnicamente al contexto recuperado.  
* **Soporte Multimodal Real:** Utilizar modelos de visiÃ³n (VLM) para generar descripciones ricas de imÃ¡genes educativas, permitiendo su recuperaciÃ³n mediante consultas textuales.  
* **Adaptabilidad de Interfaz:** Proveer una experiencia de usuario diferenciada mediante dos arquetipos de asistente:  
  * *Perfil TÃ©cnico (ArIA):* Respuestas concisas, cÃ³digo y logs.  
  * *Perfil Docente (LexIA):* Explicaciones pedagÃ³gicas y didÃ¡cticas.  
* **EvaluaciÃ³n CientÃ­fica:** Medir el rendimiento del sistema mediante mÃ©tricas objetivas (Hit Rate, MRR, RAGAS) para validar la elecciÃ³n de modelos de embeddings.  
  ---

  ## **2\. Arquitectura TÃ©cnica**

El sistema se basa en una arquitectura de microservicios desacoplada, donde el frontend (Streamlit) se comunica con el nÃºcleo lÃ³gico (FastAPI) mediante peticiones REST. El pipeline RAG implementado sigue un enfoque **hÃ­brido y multimodal**.

### 

### **2.1. Diagrama del Flujo de Datos**

### ![][image1]

### **2.2. Componentes del Pipeline**

#### **A. Fase de Ingesta (Offline)**

Antes de la ejecuciÃ³n, los datos no estructurados se procesan y almacenan:

1. **Procesamiento de Texto (PDFs):** Se extrae el contenido textual, se limpia y se fragmenta (*chunking*) en ventanas de contexto optimizadas (1000 tokens con solapamiento).  
2. **Procesamiento de ImÃ¡genes:** Se utiliza un **Modelo de VisiÃ³n-Lenguaje (VLM)** (como *LLaVA* o *Phi-3-Vision*) para generar descripciones textuales ricas de cada diagrama o diapositiva.  
3. **VectorizaciÃ³n Dual:**  
   * **Texto:** Se generan embeddings densos utilizando el modelo `Qwen/Qwen3-Embedding-0.6B`.  
   * **ImÃ¡genes:** Se generan embeddings visuales alineados semÃ¡nticamente utilizando `clip-ViT-B-32`.  
4. **Almacenamiento:** Todo se indexa en **ChromaDB**, manteniendo metadatos crÃ­ticos (asignatura, pÃ¡gina, ruta del archivo).

   #### **B. Fase de Inferencia (Online)**

Cuando el usuario realiza una pregunta:

1. **Reescritura de Consulta (Query Rewriting):** Un LLM ligero reformula la pregunta del usuario utilizando el historial del chat para resolver correferencias (ej. transformar "Â¿y sus ventajas?" en "Â¿CuÃ¡les son las ventajas de Kafka?").  
2. **RecuperaciÃ³n HÃ­brida (Hybrid Search):** Se ejecutan dos bÃºsquedas en paralelo:  
   * *BÃºsqueda Densa (Vectorial):* Recupera conceptos semÃ¡nticamente similares.  
   * *BÃºsqueda Dispersa (BM25):* Recupera coincidencias exactas de palabras clave.  
3. **FusiÃ³n de Resultados:** Se combinan ambas listas utilizando el algoritmo **Reciprocal Rank Fusion (RRF)** para obtener los candidatos mÃ¡s robustos.  
4. **Reordenamiento (Reranking):** Un modelo **Cross-Encoder** (`BAAI/bge-reranker-v2-m3`) evalÃºa la relevancia real de cada par pregunta-documento, descartando falsos positivos.  
5. **GeneraciÃ³n de Respuesta:** Se construye un prompt dinÃ¡mico inyectando el contexto recuperado y se envÃ­a al LLM principal (configurado con roles de "ArIA" o "LexIA") para generar la respuesta final en *streaming*.

## 

## 3\. TecnologÃ­as y Modelos

El desarrollo del proyecto se ha realizado utilizando un stack tecnolÃ³gico moderno, priorizando el rendimiento (baja latencia) y la precisiÃ³n en la recuperaciÃ³n de informaciÃ³n.

### 3.1. Stack TecnolÃ³gico (Core)

| Componente | TecnologÃ­a | DescripciÃ³n y Uso |
| :---- | :---- | :---- |
| **Lenguaje Base** | Python 3.10+ | Lenguaje principal por su ecosistema de IA. |
| **Frontend** | Streamlit | Interfaz grÃ¡fica rÃ¡pida para prototipado de aplicaciones de datos. |
| **Backend API** | FastAPI | Framework ASGI de alto rendimiento para exponer los endpoints del modelo. |
| **Vector Database** | ChromaDB | Base de datos vectorial *open-source* y persistente para almacenar embeddings. |
| **LibrerÃ­as RAG** | SentenceTransformers | OrquestaciÃ³n de modelos de embedding y Cross-Encoders. |
| **BÃºsqueda LÃ©xica** | Rank\_BM25 | Algoritmo probabilÃ­stico para recuperaciÃ³n por palabras clave (Sparse Retrieval). |
| **Procesamiento** | PyMuPDF / Pillow | ExtracciÃ³n de texto de PDFs y manipulaciÃ³n de imÃ¡genes. |

### 3.2. Modelos de Inteligencia Artificial

Se han seleccionado modelos especÃ­ficos tras realizar benchmarks de rendimiento (ver SecciÃ³n 6), optimizando el balance entre precisiÃ³n semÃ¡ntica y coste computacional.

| Tipo de Modelo | Modelo Seleccionado | JustificaciÃ³n TÃ©cnica |
| :---- | :---- | :---- |
| **Embedding de Texto** | `Qwen/Qwen3-Embedding-0.6B` | Modelo SOTA (State-of-the-Art) multilingÃ¼e. Supera a modelos de OpenAI en benchmarks MTEB para espaÃ±ol. |
| **Embedding de Imagen** | `clip-ViT-B-32` | Modelo de OpenAI que alinea texto e imagen en el mismo espacio vectorial, crucial para la bÃºsqueda multimodal. |
| **Reranker** | `BAAI/bge-reranker-v2-m3` | Cross-Encoder que reevalÃºa la relevancia semÃ¡ntica de los candidatos recuperados. Mejora el Hit Rate significativamente. |
| **LLM (Inferencia)** | `llama-3.3-70b-versatile` | Ejecutado vÃ­a **Groq** (LPU). Seleccionado por su velocidad de inferencia extrema (\>300 tokens/s) y capacidad de razonamiento. |
| **VLM (Ingesta)** | `llava-phi3` / `moondream` | Modelos de VisiÃ³n-Lenguaje ejecutados localmente con **Ollama** para generar descripciones densas de las imÃ¡genes durante la ingesta. |

### 3.3. Decisiones de Arquitectura

1. **Enfoque "Hybrid Search" (Denso \+ Disperso):**  
     
   * Se utiliza **BÃºsqueda Vectorial** para captar el significado semÃ¡ntico (ej. entender que "aprendizaje automÃ¡tico" es similar a "machine learning").  
   * Se utiliza **BM25** para captar coincidencias exactas de tÃ©rminos tÃ©cnicos o acrÃ³nimos (ej. "ACID", "CAP", "YARN") que los modelos vectoriales a veces diluyen.  
   * Ambos resultados se normalizan y combinan mediante **Reciprocal Rank Fusion (RRF)**.

   

2. **Estrategia Multimodal "Image-to-Text":**  
     
   * En lugar de realizar una bÃºsqueda pura de imagen-a-imagen, el sistema procesa las imÃ¡genes en la fase de ingesta generando descripciones textuales detalladas. Esto permite que una consulta de texto ("diagrama de arquitectura kafka") recupere la imagen correcta basÃ¡ndose en su contenido semÃ¡ntico descrito.

   

3. **Pipeline de Dos Etapas (Retrieval \+ Reranking):**  
     
   * *Etapa 1 (Retrieval):* RecuperaciÃ³n rÃ¡pida de 50 candidatos combinando ChromaDB y BM25.  
   * *Etapa 2 (Reranking):* AnÃ¡lisis profundo de esos 50 candidatos con el Cross-Encoder para seleccionar los 4 mejores contextualmente. Esto maximiza la precisiÃ³n sin sacrificar la latencia.

## 4\. Estructura del Proyecto

El proyecto sigue una estructura modular rigurosa, separando claramente la lÃ³gica de ingestiÃ³n de datos (ETL), el backend de inferencia, la interfaz de usuario y los mÃ³dulos de validaciÃ³n cientÃ­fica.

ğŸ“ RAG\_MULTIMODAL/  
â”œâ”€â”€ ğŸ“‚ chroma\_db\_multimodal(...)/   \# Persistencia de Vectores (Base de Datos Vectorial)  
â”œâ”€â”€ ğŸ“‚ data/                        \# Dataset Origen (Input)  
â”‚   â”œâ”€â”€ ğŸ“‚ imagenes/                \# Diapositivas, diagramas y esquemas (.png, .jpg)  
â”‚   â””â”€â”€ ğŸ“‚ pdfs/                    \# Apuntes tÃ©cnicos y documentaciÃ³n (.pdf)  
â”œâ”€â”€ ğŸ“‚ img/                         \# Assets estÃ¡ticos de la UI (logos, avatares)  
â”œâ”€â”€ ğŸ“‚ src/                         \# CÃ³digo Fuente Principal  
â”‚   â”œâ”€â”€ ğŸ“œ config.py                \# ConfiguraciÃ³n global y gestiÃ³n de variables de entorno  
â”‚   â”‚  
â”‚   â”œâ”€â”€ ğŸ“‚ api/  
â”‚   â”‚   â””â”€â”€ ğŸ“œ api.py               \# Backend FastAPI: NÃºcleo lÃ³gico del RAG y Endpoints  
â”‚   â”œâ”€â”€ ğŸ“‚ app/  
â”‚   â”‚   â””â”€â”€ ğŸ“œ app.py               \# Frontend Streamlit: Interfaz de Chat y GestiÃ³n de Estado  
â”‚   â”‚  
â”‚   â”‚   \# \--- PIPELINE DE INGESTA (ETL) \---  
â”‚   â”œâ”€â”€ ğŸ“œ 01\_multimodal\_ingest\_smart.py  \# Procesamiento de imÃ¡genes y embeddings  
â”‚   â”œâ”€â”€ ğŸ“œ 02\_ingest\_pdfs.py              \# Procesamiento: Limpieza, Chunking etcâ€¦rizaciÃ³n  
â”‚   â”œâ”€â”€ ğŸ“œ 03\_check\_chroma\_content.py     \# DiagnÃ³stico para inspeccionar la DB  
â”‚   â”‚  
â”‚   â”‚   \# \--- SUITE DE EVALUACIÃ“N Y BENCHMARKING \---  
â”‚   â”œâ”€â”€ ğŸ“œ 04\_resultados.py         \# VisualizaciÃ³n del espacio latente (ProyecciÃ³n t-SNE)  
â”‚   â”œâ”€â”€ ğŸ“œ 05\_comprobar.py          \# A/B Testing: Comparativa de modelos de texto  
â”‚   â”œâ”€â”€ ğŸ“œ 05\_comprobar\_imagenes.py \# A/B: Impacto idioma en recuperaciÃ³n visual  
â”‚   â”œâ”€â”€ ğŸ“œ 06\_buscar\_imagen.py      \# DepuraciÃ³n para bÃºsqueda visual inversa  
â”‚   â”œâ”€â”€ ğŸ“œ 07\_eval\_retrieval.py     \# CÃ¡lculo de mÃ©tricas de recuperaciÃ³n (Hit Rate)  
â”‚   â”œâ”€â”€ ğŸ“œ 08\_ragas.py              \# EvaluaciÃ³n de respuestas con RAGAS  
â”‚   â””â”€â”€ ğŸ“œ 09\_evaluar\_metricas.py   \# (Chunk Size vs Reranking)  
â”‚  
â”œâ”€â”€ ğŸ“œ .env                         \# Credenciales y claves API (No incluido en repo)  
â”œâ”€â”€ ğŸ“œ requirements.txt             \# Lista de dependencias y versiones  
â””â”€â”€ ğŸ“œ README.md                    \# DocumentaciÃ³n tÃ©cnica del proyecto

### **4.1. DescripciÃ³n de MÃ³dulos Clave**

* **`src/api/api.py` (Backend):** Es el orquestador del sistema. Recibe la consulta del usuario, ejecuta la reescritura de la pregunta, lanza la bÃºsqueda hÃ­brida en ChromaDB y BM25, aplica el reranking con Cross-Encoders y gestiona el streaming de la respuesta generada por el LLM.  
* **`src/app/app.py` (Frontend):** Gestiona la experiencia de usuario. Controla la sesiÃ³n, el historial de chat, la renderizaciÃ³n de imÃ¡genes recuperadas y la lÃ³gica de personalidades (ArIA/LexIA) mediante inyecciÃ³n de CSS dinÃ¡mico.  
* **`src/01_multimodal_ingest_smart.py`:** Componente crÃ­tico de la multimodalidad. Utiliza un modelo de visiÃ³n local para "ver" y describir textualmente cada imagen del dataset antes de vectorizarla. Esto permite que las imÃ¡genes sean recuperables mediante bÃºsquedas semÃ¡nticas de texto.  
* **`src/09_evaluar_metricas.py`:** Script cientÃ­fico utilizado para validar la arquitectura. Ejecuta pruebas automatizadas variando parÃ¡metros (tamaÃ±o de chunk, uso de reranker) para generar las mÃ©tricas de rendimiento (Hit Rate, MRR, Latencia) presentadas en este documento.
