# **Asistente Virtual RAG Multimodal: Especializaci√≥n en IA y Big Data**

**Sistema de Recuperaci√≥n Aumentada por Generaci√≥n (RAG) con capacidades multimodales (Texto \+ Imagen) para la gesti√≥n del conocimiento acad√©mico.**

## **1\. Descripci√≥n del Proyecto**

Este repositorio contiene la implementaci√≥n completa de un asistente virtual t√©cnico, dise√±ado para resolver la fragmentaci√≥n y dispersi√≥n de la informaci√≥n habitual en el entorno educativo. Con frecuencia, el material de estudio se encuentra segregado en plataformas educativas, distribuido en m√∫ltiples carpetas, PDFs extensos y diapositivas sueltas, lo que obliga al alumno a abrir y buscar manualmente en docenas de archivos para encontrar un concepto espec√≠fico. Este sistema centraliza y unifica todos estos recursos (apuntes t√©cnicos e im√°genes) en una √∫nica base de conocimiento, permitiendo a los estudiantes interactuar en lenguaje natural y obtener respuestas precisas sin necesidad de navegar por la compleja estructura de archivos original.

A diferencia de los LLMs generales, este sistema opera bajo un esquema de **Dominio Cerrado**: las respuestas se generan exclusivamente a partir de la documentaci√≥n indexada, eliminando las alucinaciones y garantizando la trazabilidad de la informaci√≥n mediante citas expl√≠citas a las fuentes.

La soluci√≥n integra un pipeline avanzado de **B√∫squeda H√≠brida** (Sem√°ntica \+ Palabras Clave) y un sistema de **Reordenamiento (Reranking)**, optimizado espec√≠ficamente para el idioma espa√±ol y terminolog√≠a t√©cnica de Ingenier√≠a de Datos.

### **1.1. Motivaci√≥n y Problema a Resolver**

En asignaturas t√©cnicas como *Big Data* o *Inteligencia Artificial*, el material de estudio suele estar disperso en m√∫ltiples formatos:

* **Texto denso:** Manuales de referencia y papers en PDF.  
* **Informaci√≥n visual cr√≠tica:** Diagramas de flujo (ej. arquitectura Kafka), capturas de c√≥digo y esquemas conceptuales que los LLMs de texto tradicionales ignoran.

**El problema:** Los estudiantes pierden tiempo buscando referencias cruzadas y los modelos est√°ndar fallan al interpretar preguntas que requieren contexto visual espec√≠fico (ej. "¬øQu√© representa el bloque azul en el diagrama de arquitectura de Hadoop?").

**Nuestra soluci√≥n:** Un motor RAG Multimodal que vectoriza tanto el texto como las descripciones sem√°nticas de las im√°genes, permitiendo una recuperaci√≥n de informaci√≥n hol√≠stica.

### **1.2. Objetivos Principales**

* **Centralizaci√≥n del Conocimiento:** Unificar fuentes en una √∫nica base de datos vectorial consultable (ChromaDB).  
* **Precisi√≥n T√©cnica (Zero-Hallucination):** Implementar *Guardrails* estrictos en el prompt del sistema para restringir las respuestas √∫nicamente al contexto recuperado.  
* **Soporte Multimodal Real:** Utilizar modelos de visi√≥n (VLM) para generar descripciones ricas de im√°genes educativas, permitiendo su recuperaci√≥n mediante consultas textuales.  
* **Adaptabilidad de Interfaz:** Proveer una experiencia de usuario diferenciada mediante dos arquetipos de asistente:  
  * *Perfil T√©cnico (ArIA):* Respuestas concisas, c√≥digo y logs.  
  * *Perfil Docente (LexIA):* Explicaciones pedag√≥gicas y did√°cticas.  
* **Evaluaci√≥n:** Medir el rendimiento del sistema mediante m√©tricas objetivas (Hit Rate, MRR, RAGAS) para validar la elecci√≥n de modelos de embeddings.  
  ---

  ## **2\. Arquitectura T√©cnica**

El sistema se basa en una arquitectura de servicios desacoplada, donde el frontend (Streamlit) se comunica con el n√∫cleo (FastAPI) mediante peticiones REST. El pipeline RAG implementado sigue un enfoque **h√≠brido y multimodal**.

### 

### **2.1. Diagrama del Flujo de Datos**

<p align="center">
  <img src="img/esquema.png" alt="Esquema de Arquitectura" width="30%">
</p>

### **2.2. Componentes del Pipeline**

#### **A. Fase de Ingesta (Offline)**

Antes de la ejecuci√≥n, los datos no estructurados se procesan y almacenan:

1. **Procesamiento de Texto (PDFs):** Se extrae el contenido textual, se limpia y se fragmenta (*chunking*) en ventanas de contexto optimizadas (1000 tokens con solapamiento).  
2. **Procesamiento de Im√°genes:** Se utiliza un **Modelo de Visi√≥n-Lenguaje (VLM)** (*LLaVA*) para generar descripciones textuales ricas de cada diagrama o diapositiva.  
3. **Vectorizaci√≥n Dual:**  
   * **Texto:** Se generan embeddings densos utilizando el modelo `Qwen/Qwen3-Embedding-0.6B`.  
   * **Im√°genes:** Se generan embeddings visuales utilizando `clip-ViT-B-32`.  
4. **Almacenamiento:** Todo se indexa en **ChromaDB**, manteniendo metadatos cr√≠ticos (asignatura, p√°gina, ruta del archivo).

   #### **B. Fase de Inferencia (Online)**

Cuando el usuario realiza una pregunta:

1. **Reescritura de Consulta (Query Rewriting):** Un LLM ligero reformula la pregunta del usuario utilizando el historial del chat para resolver correferencias (ej. transformar "¬øy sus ventajas?" en "¬øCu√°les son las ventajas de Kafka?").  
2. **Recuperaci√≥n H√≠brida (Hybrid Search):** Se ejecutan dos b√∫squedas en paralelo:  
   * *B√∫squeda Densa (Vectorial):* Recupera conceptos sem√°nticamente similares.  
   * *B√∫squeda Dispersa (BM25):* Recupera coincidencias exactas de palabras clave.  
3. **Fusi√≥n de Resultados:** Se combinan ambas listas utilizando el algoritmo **Reciprocal Rank Fusion (RRF)** para obtener los candidatos m√°s robustos.  
4. **Reordenamiento (Reranking):** Un modelo **Cross-Encoder** (`BAAI/bge-reranker-v2-m3`) eval√∫a la relevancia real de cada par pregunta-documento, descartando falsos positivos.  
5. **Generaci√≥n de Respuesta:** Se construye un prompt din√°mico inyectando el contexto recuperado y se env√≠a al LLM principal (configurado con roles de "ArIA" o "LexIA") para generar la respuesta final en *streaming*.

## 

## 3\. Tecnolog√≠as y Modelos

El desarrollo del proyecto se ha realizado utilizando un stack tecnol√≥gico moderno, priorizando el rendimiento (baja latencia) y la precisi√≥n en la recuperaci√≥n de informaci√≥n.

### 3.1. Stack Tecnol√≥gico (Core)

| Componente | Tecnolog√≠a | Descripci√≥n y Uso |
| :---- | :---- | :---- |
| **Lenguaje Base** | Python 3.10+ | Lenguaje principal por su ecosistema de IA. |
| **Frontend** | Streamlit | Interfaz gr√°fica r√°pida para prototipado de aplicaciones de datos. |
| **Backend API** | FastAPI | Framework ASGI de alto rendimiento para exponer los endpoints del modelo. |
| **Vector Database** | ChromaDB | Base de datos vectorial *open-source* y persistente para almacenar embeddings. |
| **Librer√≠as RAG** | SentenceTransformers | Orquestaci√≥n de modelos de embedding y Cross-Encoders. |
| **B√∫squeda L√©xica** | Rank\_BM25 | Algoritmo probabil√≠stico para recuperaci√≥n por palabras clave (Sparse Retrieval). |
| **Procesamiento** | PyMuPDF / Pillow | Extracci√≥n de texto de PDFs y manipulaci√≥n de im√°genes. |

### 3.2. Modelos de Inteligencia Artificial

Se han seleccionado modelos espec√≠ficos tras realizar benchmarks de rendimiento (ver Secci√≥n 6), optimizando el balance entre precisi√≥n sem√°ntica y coste computacional.

| Tipo de Modelo | Modelo Seleccionado | Justificaci√≥n T√©cnica |
| :---- | :---- | :---- |
| **Embedding de Texto** | `Qwen/Qwen3-Embedding-0.6B` | Modelo SOTA (State-of-the-Art) multiling√ºe. Supera a modelos de OpenAI en benchmarks MTEB para espa√±ol. |
| **Embedding de Imagen** | `clip-ViT-B-32` | Modelo de OpenAI que alinea texto e imagen en el mismo espacio vectorial, crucial para la b√∫squeda multimodal. |
| **Reranker** | `BAAI/bge-reranker-v2-m3` | Cross-Encoder que reeval√∫a la relevancia sem√°ntica de los candidatos recuperados. Mejora el Hit Rate significativamente. |
| **LLM (Inferencia)** | `llama-3.3-70b-versatile` | Ejecutado v√≠a **Groq** (LPU). Seleccionado por su velocidad de inferencia extrema (\>300 tokens/s) y capacidad de razonamiento. |
| **VLM (Ingesta)** | `llava-phi3` | Modelo de Visi√≥n-Lenguaje ejecutado localmente con **Ollama** para generar descripciones densas de las im√°genes durante la ingesta. |

### 3.3. Decisiones de Arquitectura

1. **Enfoque "Hybrid Search" (Denso \+ Disperso):**  
     
   * Se utiliza **B√∫squeda Vectorial** para captar el significado sem√°ntico (ej. entender que "aprendizaje autom√°tico" es similar a "machine learning").  
   * Se utiliza **BM25** para captar coincidencias exactas de t√©rminos t√©cnicos o acr√≥nimos (ej. "ACID", "CAP", "YARN") que los modelos vectoriales a veces diluyen.  
   * Ambos resultados se normalizan y combinan mediante **Reciprocal Rank Fusion (RRF)**.

   

2. **Estrategia Multimodal "Image-to-Text":**  
     
   * En lugar de realizar una b√∫squeda pura de imagen-a-imagen, el sistema procesa las im√°genes en la fase de ingesta generando descripciones textuales detalladas. Esto permite que una consulta de texto ("diagrama de arquitectura kafka") recupere la imagen correcta bas√°ndose en su contenido sem√°ntico descrito.

   

3. **Pipeline de Dos Etapas (Retrieval \+ Reranking):**  
     
   * *Etapa 1 (Retrieval):* Recuperaci√≥n r√°pida de 50 candidatos combinando ChromaDB y BM25.  
   * *Etapa 2 (Reranking):* An√°lisis profundo de esos 50 candidatos con el Cross-Encoder para seleccionar los 4 mejores contextualmente. Esto maximiza la precisi√≥n sin sacrificar la latencia.

## 4\. Estructura del Proyecto

El proyecto sigue una estructura modular rigurosa, separando claramente la l√≥gica de ingesti√≥n de datos (ETL), el backend de inferencia, la interfaz de usuario y los m√≥dulos de validaci√≥n cient√≠fica.

```ini
RAG_MULTIMODAL/
‚îú‚îÄ‚îÄ chroma_db_multimodal/      # Base de Datos Vectorial (Persistencia)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ imagenes/              # Dataset Im√°genes (.png, .jpg)
‚îÇ   ‚îî‚îÄ‚îÄ pdfs/                  # Dataset PDFs (Apuntes)
‚îú‚îÄ‚îÄ img/                       # Logos y avatares UI
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuraci√≥n Global
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.py             # Backend FastAPI (L√≥gica RAG)
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app.py             # Frontend Streamlit (Chat UI)
‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   # --- INGESTA (ETL) ---
‚îÇ   ‚îú‚îÄ‚îÄ 01_multimodal_ingest.py # Procesar Im√°genes + Embeddings
‚îÇ   ‚îú‚îÄ‚îÄ 02_ingest_pdfs.py       # Procesar PDFs + Vectorizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ 03_check_chroma.py      # Diagn√≥stico de la DB
‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   # --- EVALUACI√ìN ---
‚îÇ   ‚îú‚îÄ‚îÄ 04_resultados.py        # Visualizaci√≥n (t-SNE)
‚îÇ   ‚îú‚îÄ‚îÄ 05_comprobar.py         # Test A/B Texto
‚îÇ   ‚îú‚îÄ‚îÄ 05_comprobar_img.py     # Test A/B Im√°genes
‚îÇ   ‚îú‚îÄ‚îÄ 06_buscar_imagen.py     # Debug B√∫squeda Visual
‚îÇ   ‚îú‚îÄ‚îÄ 07_eval_retrieval.py    # M√©tricas Hit Rate
‚îÇ   ‚îú‚îÄ‚îÄ 08_ragas.py             # Eval Sem√°ntica RAGAS
‚îÇ   ‚îî‚îÄ‚îÄ 09_evaluar_metricas.py  # Benchmark Arquitectura
‚îÇ
‚îú‚îÄ‚îÄ .env                        # Claves API
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias
‚îî‚îÄ‚îÄ README.md                   # Documentaci√≥n
```
### **4.1. Descripci√≥n de Scripts Clave**

* **`src/api/api.py` (Backend):** Es el orquestador del sistema. Recibe la consulta del usuario, ejecuta la reescritura de la pregunta, lanza la b√∫squeda h√≠brida en ChromaDB y BM25, aplica el reranking con Cross-Encoders y gestiona el streaming de la respuesta generada por el LLM.  
* **`src/app/app.py` (Frontend):** Gestiona la experiencia de usuario. Controla la sesi√≥n, el historial de chat, la visualizaci√≥n de im√°genes recuperadas y la l√≥gica de personalidades (ArIA/LexIA) mediante inyecci√≥n de CSS din√°mico.  
* **`src/01_multimodal_ingest_smart.py`:** Componente cr√≠tico de la multimodalidad. Utiliza un modelo de visi√≥n local para "ver" y describir textualmente cada imagen del dataset antes de vectorizarla. Esto permite que las im√°genes sean recuperables mediante b√∫squedas sem√°nticas de texto.  
* **`src/02_ingest_pdfs.py`:** Motor de ingesta de documentos. Se encarga de extraer, limpiar y fragmentar (chunking) el contenido de los apuntes en PDF. Posteriormente, vectoriza estos fragmentos y los almacena en ChromaDB, constituyendo el n√∫cleo de conocimiento textual del sistema.

## 5\. Instalaci√≥n y Uso

Sigue estos pasos para desplegar el entorno de desarrollo local y ejecutar el asistente.

### 5.1. Prerrequisitos

* Anaconda o Miniconda instalado.  
* Python 3.10 o superior.  
* Git.

### 5.2. Configuraci√≥n del Entorno

1. **Clonar el repositorio:**  
     
   git clone \[https://github.com/tu-usuario/rag-multimodal.git\](https://github.com/tu-usuario/rag-multimodal.git)  
     
   cd rag-multimodal  
     
2. **Crear y activar el entorno virtual:**  
     
   conda create \--name rag\_multimodal python=3.10 \-y  
     
   conda activate rag\_multimodal  
     
3. **Instalar dependencias:**  
     
   pip install \-r requirements.txt  
     
4. **Variables de Entorno (.env):**
   Crea un archivo `.env` en la ra√≠z del proyecto y configura tus claves API. Usa la siguiente plantilla basada en la configuraci√≥n actual:

```ini
   # --- API KEYS (Requerido) ---
   OPENROUTER_API_KEY="sk-or-..."
   GROQ_API_KEY="gsk_..."

   # --- CONFIGURACI√ìN LLM (Cerebro) ---
   # Opciones: 'groq' o 'openrouter'
   LLM_PROVIDER="groq"
   
   # Modelos Espec√≠ficos
   LLM_MODEL_OPENROUTER="deepseek/deepseek-r1:free"
   LLM_MODEL_GROQ="llama-3.3-70b-versatile"

   # --- RUTAS DE DATOS (PATHS) ---
   DB_PATH="./chroma_db_multimodal(casa_llava_qwen)buena_spanish"
   DATA_PATH_IMAGENES="./data/imagenes"
   DATA_PATH_PDFS="./data/pdfs"

   # --- MODELOS LOCALES (Embeddings & Reranker) ---
   MODEL_EMBEDDING_TEXT="Qwen/Qwen3-Embedding-0.6B"
   MODEL_EMBEDDING_IMAGE="clip-ViT-B-32"
   MODEL_RERANKER="BAAI/bge-reranker-v2-m3"

   # --- PAR√ÅMETROS T√âCNICOS ---
   API_HOST="127.0.0.1"
   API_PORT="8000"
   UMBRAL_RERANKER="0.0"
```

### 5.3. Ejecuci√≥n del Sistema

El sistema requiere una fase inicial de preparaci√≥n de datos (Ingesta) antes de poder realizar consultas.

#### Fase 1: Ingesta de Datos (Ejecutar solo al inicio o al actualizar apuntes)

1. **Procesar Im√°genes (Multimodal):** Genera descripciones textuales de las im√°genes ubicadas en `./data/imagenes` utilizando un VLM local.  
     
   python src/01\_multimodal\_ingest\_smart.py  
     
2. **Procesar Documentos (PDFs):** Limpia, fragmenta y vectoriza los PDFs ubicados en `./data/pdfs`.  
     
   python src/02\_ingest\_pdfs.py

#### Fase 2: Lanzamiento de la Aplicaci√≥n

Para utilizar el asistente, es necesario ejecutar el Backend y el Frontend en **dos terminales separadas**:

**Terminal 1: Backend (API)** Inicia el servidor l√≥gico que gestiona la IA y la base de datos.

uvicorn src.api.api:app --reload

*Esperar hasta ver el mensaje: `[LISTO] Sistema preparado para consultas.`*

**Terminal 2: Frontend (UI)** Inicia la interfaz gr√°fica de usuario.

streamlit run src/app/app.py

Una vez iniciados ambos servicios, la aplicaci√≥n se abrir√° autom√°ticamente en tu navegador predeterminado en: [http://localhost:8501](http://localhost:8501).

## 6\. Evaluaci√≥n y M√©tricas

Para garantizar la fiabilidad del asistente en un entorno real, se ha sometido el sistema a una bater√≠a de pruebas rigurosas, evaluando tanto la capacidad de recuperaci√≥n (Retrieval) como la calidad de la generaci√≥n (Generation).

### 6.1. Comparativa de Modelos de Embeddings

Se evalu√≥ el rendimiento de distintos modelos para determinar cu√°l capturaba mejor la sem√°ntica en espa√±ol del dominio t√©cnico.

| Modelo de Embedding | Rendimiento (Accuracy) | Observaciones |
| :---- | :---: | :---- |
| `intfloat/multilingual-e5-large` | 80.00% | Buen rendimiento general, pero falla en terminolog√≠a espec√≠fica. |
| **`Qwen/Qwen3-Embedding-0.6B`** | **90.00%** | **Seleccionado.** Superior en comprensi√≥n de instrucciones y contexto t√©cnico en espa√±ol. |

**Impacto del Idioma en Multimodalidad:** Para la recuperaci√≥n de im√°genes, se analiz√≥ c√≥mo afecta el idioma de la descripci√≥n generada por el VLM (Vision-Language Model).

| Configuraci√≥n de Imagen | Rendimiento | Conclusi√≥n |
| :---- | :---: | :---- |
| Descripciones en Ingl√©s (Raw) | 85.00% | El modelo de embedding pierde matices al cruzar idiomas. |
| **Descripciones en Espa√±ol** | **95.00%** | La alineaci√≥n ling√º√≠stica entre la consulta del usuario y la descripci√≥n de la imagen es cr√≠tica. |

### 6.2. Evaluaci√≥n de Arquitectura (Chunking & Reranking)

Se realizaron pruebas A/B variando el tama√±o de fragmentaci√≥n del texto (Chunk Size) y activando/desactivando el reordenamiento neuronal (Reranker).

**M√©tricas utilizadas:**

* **Hit Rate@3:** Probabilidad de que el documento correcto est√© en el Top 3\.  
* **MRR@3 (Mean Reciprocal Rank):** Calidad del ordenamiento (cuanto m√°s cerca de 1, mejor).  
* **Latencia:** Tiempo promedio de procesamiento por consulta.

| Configuraci√≥n | Hit Rate@3 | MRR@3 | Latencia (s) | An√°lisis |
| :---- | :---: | :---: | :---: | :---- |
| `db_800` (Base) | 76.9% | 0.73 | **0.335s** | Muy r√°pido, pero precisi√≥n mejorable. |
| `db_800` (+Reranker) | 84.6% | 0.77 | 5.083s | Mejora notable en recuperaci√≥n. |
| `db_1000` (Base) | 76.9% | 0.68 | **0.328s** | Similar al base de 800 tokens. |
| **`db_1000` (+Reranker)** | **84.6%** | **0.78** | 5.861s | **Configuraci√≥n √ìptima.** M√°xima precisi√≥n sem√°ntica (MRR), aceptando una mayor latencia. |

**Conclusi√≥n T√©cnica:** La incorporaci√≥n del **Cross-Encoder (Reranker)** es fundamental. Aunque introduce una latencia de \~5 segundos, eleva la precisi√≥n del sistema del 76% al **84.6%**, lo cual es cr√≠tico para evitar alucinaciones en respuestas t√©cnicas.

### 

### 6.3. Calidad Sem√°ntica (Framework RAGAS)

Para evaluar la respuesta final generada por el LLM, se utiliz√≥ el framework RAGAS, que utiliza un "Juez IA" (GPT-4 / Llama-3) para puntuar la calidad.

| M√©trica | Puntuaci√≥n (0-1) | Interpretaci√≥n |
| :---- | :---: | :---- |
| **Faithfulness** | **0.905** | **Alta.** El sistema apenas alucina; las respuestas se basan casi exclusivamente en el contexto recuperado (apuntes). |
| **Answer Relevancy** | **0.939** | **Excelente.** El asistente responde exactamente a la intenci√≥n de la pregunta del usuario. |
| **Context Precision** | **0.706** | **Buena.** El sistema recupera mayoritariamente informaci√≥n √∫til, aunque a veces incluye algo de "ruido" (contexto irrelevante) que el LLM debe filtrar. |

**Validaci√≥n:** Los resultados demuestran que el sistema es **robusto y confiable** para su uso como tutor acad√©mico, priorizando la veracidad de la informaci√≥n (Faithfulness) sobre la creatividad.

## 7\. Funcionalidades del Sistema

El asistente ha sido dise√±ado no solo como un motor de b√∫squeda, sino como una herramienta de estudio interactiva con capacidades avanzadas de adaptaci√≥n al usuario.

### 7.1. Personalizaci√≥n de la Experiencia (Dual Persona)

El sistema implementa dos arquetipos de asistente distintos, seleccionables desde la barra lateral. Esta funcionalidad altera tanto el **Prompt del Sistema (Backend)** como la **Interfaz Gr√°fica (Frontend)** mediante inyecci√≥n din√°mica de CSS.

| Caracter√≠stica | üë®‚Äçüíª Modo ArIA (T√©cnico) | üë©‚Äçüè´ Modo LexIA (Docente) |
| :---- | :---- | :---- |
| **Rol** | Ingeniero de Sistemas Senior. | Catedr√°tica Universitaria. |
| **Objetivo** | Eficiencia y precisi√≥n t√©cnica. | Pedagog√≠a y comprensi√≥n profunda. |
| **Estilo de Respuesta** | Conciso, uso intensivo de *bullet points*, bloques de c√≥digo y terminolog√≠a experta. | Explicativo, uso de analog√≠as, tono amable y estructuraci√≥n did√°ctica. |
| **Interfaz (UI)** | Tema "Hacker/Terminal" (Fuente Monospace, Acentos Azul Ne√≥n). | Tema "Acad√©mico/Paper" (Fuente Serif, Acentos Violeta/Lavanda). |
| **Gesti√≥n de Errores** | Reportes de error t√©cnicos (Logs). | Mensajes de ayuda y reorientaci√≥n. |

### 7.2. Recuperaci√≥n Multimodal (Texto \+ Imagen)

El sistema rompe la barrera del texto plano al integrar recursos visuales en las respuestas:

* **Indexaci√≥n Sem√°ntica de Im√°genes:** Las im√°genes no se recuperan por nombre de archivo, sino por su contenido visual (interpretado por modelos VLM durante la ingesta).  
* **Renderizado Contextual:** Si la respuesta a una pregunta (ej: "Arquitectura de Spark") se entiende mejor con un diagrama, el sistema recupera la imagen correspondiente y la muestra junto a la explicaci√≥n textual.  
* **Depuraci√≥n Visual:** En el frontend, se incluye un expansor "Debug/Kernel" que muestra qu√© im√°genes fueron consideradas candidatas y su puntuaci√≥n de similitud.

### 7.3. Seguridad y Control de Alucinaciones

Para garantizar la idoneidad acad√©mica, se han implementado estrictos *Guardrails* en el prompt del sistema:

1. **Protocolo "Zero-Hallucination":** Si la informaci√≥n no existe en la base de datos vectorial (apuntes), el modelo tiene prohibido inventar una respuesta o utilizar conocimiento externo generalista.  
2. **Filtrado de Dominio:** El asistente rechaza consultas fuera del √°mbito acad√©mico (ej: recetas de cocina, opiniones deportivas), manteniendo el foco en la materia de estudio.  
3. **Gesti√≥n de Rate Limits:** El sistema captura y gestiona proactivamente los errores de cuota de la API (Error 429), informando al usuario con mensajes amigables en lugar de fallos t√©cnicos.

## 

## 8\. Autores y Licencia

Este proyecto ha sido desarrollado como parte del RETO 2 de la Especializaci√≥n en Inteligencia Artificial y Big Data.

### üë• Autores

* **Zuri√±e Colino** \- *Analista de Datos & IA*
* **Aritz Monje** \- *Analista de Datos & IA*

### üìÑ Licencia

Este proyecto est√° bajo la Licencia **MIT** \- mira el archivo [LICENSE.md](http://LICENSE.md) para m√°s detalles.

---

**Nota:** Este repositorio es de car√°cter acad√©mico y demostrativo. Los documentos PDF e im√°genes utilizados en el dataset `data/` son propiedad de sus respectivos autores y se utilizan aqu√≠ √∫nicamente con fines educativos bajo el concepto de *Fair Use*.
